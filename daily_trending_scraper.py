# daily_trending_scraper.py
# ğŸ“Š GitHub Trending è‡ªå‹•çˆ¬èŸ²ï¼Œæ¯æ—¥å‚™ä»½èˆ‡è¨˜éŒ„

from loguru import logger
import requests
from bs4 import BeautifulSoup
from datetime import datetime
import shutil
import os

# åŠ å…¥æ—¥èªŒç´€éŒ„å™¨
logger.add("log.txt", rotation="1 week", encoding="utf-8", enqueue=True)

def fetch_trending():
    """å¾ GitHub Trending é é¢æŠ“å– HTML"""
    url = "https://github.com/trending"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/122.0.0.0 Safari/537.36"
    }
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        logger.info("âœ… æˆåŠŸå–å¾— GitHub Trending é é¢")
        return response.text
    except Exception as e:
        logger.error(f"âŒ ç„¡æ³•å–å¾— GitHub Trending é é¢: {e}")
        return None

def parse_trending(html):
    """è§£æ HTML ä¸¦æ“·å– trending å€‰åº«è³‡æ–™"""
    soup = BeautifulSoup(html, "html.parser")
    repo_items = soup.select("article.Box-row")
    logger.info(f"ğŸ” æ‰¾åˆ° {len(repo_items)} å€‹ trending å€‰åº«")
    repos = []

    if not repo_items:
        logger.warning("âš ï¸ æ‰¾ä¸åˆ° trending å€‰åº«ï¼ŒGitHub é é¢çµæ§‹å¯èƒ½æ”¹è®Š")
        return repos

    for repo_item in repo_items:
        try:
            h1_tag = repo_item.select_one("h2 a") or repo_item.h1
            if not h1_tag:
                continue
            title = h1_tag.text.strip().replace("\n", " ").replace("  ", " ")
            link = "https://github.com" + h1_tag.get("href", "")
            description_tag = repo_item.find("p")
            description = description_tag.text.strip() if description_tag else "No description"
            repos.append((title, link, description))
        except Exception as e:
            logger.error(f"âŒ è§£æå€‰åº«è³‡æ–™å¤±æ•—: {e}")
    return repos

def backup_previous_file():
    """å‚™ä»½å‰ä¸€å¤©çš„ markdown æª”æ¡ˆ"""
    today = datetime.now().strftime("%Y-%m-%d")
    backup_dir = "backups"
    os.makedirs(backup_dir, exist_ok=True)
    source_file = "trending.md"
    backup_file = os.path.join(backup_dir, f"trending_{today}.md")
    if os.path.exists(source_file):
        shutil.copy2(source_file, backup_file)
        logger.info(f"ğŸ’¾ å‚™ä»½ trending.md åˆ° {backup_file}")

def write_to_markdown(repos):
    """è¼¸å‡º trending è³‡æ–™åˆ° markdown æª”"""
    backup_previous_file()
    today = datetime.now().strftime("%Y-%m-%d")
    filename = "trending.md"
    with open(filename, "w", encoding="utf-8") as f:
        f.write(f"# GitHub Trending Repos - {today}\n\n")
        if repos:
            for idx, (title, link, description) in enumerate(repos, 1):
                f.write(f"{idx}. [{title}]({link})\n   - {description}\n\n")
        else:
            f.write("*No trending repositories found today. This may be due to a GitHub page update.*\n")
    logger.info(f"âœ… å·²è¼¸å‡º markdown æª”ï¼š{filename}")

if __name__ == "__main__":
    html = fetch_trending()
    if html:
        trending_repos = parse_trending(html)
        write_to_markdown(trending_repos)
    else:
        logger.error("âš ï¸ æœªå–å¾— HTML è³‡æ–™ï¼Œåœæ­¢è¼¸å‡º")
